{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas DataFrame\n",
    "- It is an equvivalant of excel of sql table in pandas\n",
    "- it is a collection of series\n",
    "- It has two axes row and column. row axes is denoted by 0 and column axes is denoted by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# create a dataframe\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4],\n",
    "    'B': [10, 20, 30, 40],\n",
    "    'C': [100, 200, 300, 400]\n",
    "})\n",
    "\n",
    "# print the dataframe\n",
    "print(df)\n",
    "\n",
    "# creating a dataframe by reading a csv file\n",
    "df = pd.read_csv('/Users/anand/learning/python/pylearn/notebooks/datascience/pandas/oil.csv',)\n",
    "# removing rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "df.shape\n",
    "# verifying that there is no missing value in the dataframe\n",
    "df.isna().sum()\n",
    "print(df.count())\n",
    "print(\"good oil\")\n",
    "good_oil = df.where(lambda x: x.dcoilwtico >= 50.0).dropna()\n",
    "print(good_oil.count())\n",
    "\n",
    "bad_oil = df.where(lambda x: x.dcoilwtico < 50.0).dropna()\n",
    "print(\"bad oil\")\n",
    "print(bad_oil.count())\n",
    "\n",
    "print(\"mean of good oil\")\n",
    "newDf = good_oil.drop(columns=['date'], axis=1)\n",
    "print(newDf.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "properties of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# properties of the dataframe\n",
    "print(f'shape: {df.shape}')\n",
    "print(f'columns: {df.columns}')\n",
    "print(f'index: {df.index}')\n",
    "print(f'dtypes: {df.dtypes}')\n",
    "print(f'axeses: {df.axes}')\n",
    "# print(\"First 2 rows\")\n",
    "# print(df.head(2))\n",
    "# print(\"Last 2 rows\")\n",
    "# print(df.tail(2))\n",
    "# print(\"random 2 samples\")\n",
    "# print(df.sample(2))\n",
    "print(\"Info\")\n",
    "print(df.info(show_counts=True))\n",
    "print(\"statistics about the data\")\n",
    "print(df.describe(percentiles=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], include='all').round())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# df columns can be accessed using the dot notation as well\n",
    "# column names should follow the python variable naming rules for the dot notation to work\n",
    "stores_df = pd.read_csv('/Users/anand/learning/python/pylearn/notebooks/datascience/pandas/stores.csv')\n",
    "print(stores_df.head(2))\n",
    "\n",
    "# you can also slice the df using the column names\n",
    "subStoresdf = stores_df[['store_nbr', 'cluster']].iloc[0:5]\n",
    "print(subStoresdf.head(2))\n",
    "\n",
    "# you can rename the columns by modifying the columns attribute\n",
    "stores_df.columns = ['store_number', 'City_name', 'state', 'store_type', 'cluster']\n",
    "print(stores_df.head(2))\n",
    "\n",
    "# you can also rename the columns using the rename method\n",
    "stores_df.rename(columns={'store_number': 'store_nbr', 'City_name': 'city'}, inplace=True)\n",
    "print(stores_df.head(2))\n",
    "\n",
    "# iloc for df\n",
    "# iloc is used to access the rows and columns by their integer index\n",
    "# iloc[row_index, column_index]\n",
    "# iloc is exclusive of the end index\n",
    "print(stores_df.iloc[0:2, 0:2])\n",
    "# loc is used to access the rows and columns by their labels\n",
    "# loc[row_label, column_label]\n",
    "# loc is inclusive of the end index\n",
    "print(stores_df.loc[0:2, 'store_nbr':'state'])\n",
    "\n",
    "# drop columns\n",
    "# drop columns using the drop method\n",
    "# drop method returns a new df with the columns dropped\n",
    "# to drop the columns in place, use the inplace parameter\n",
    "stores_df.drop(columns=['state', 'store_type'], inplace=True)\n",
    "print(stores_df.head(2))\n",
    "# droppping first 10 rows\n",
    "# generally rows are dropped using slicing\n",
    "stores_df.drop(range(10), axis= 0, inplace=True)\n",
    "# duplicate the last row\n",
    "stores_df = pd.concat([stores_df, stores_df.tail(1)])\n",
    "print(stores_df.shape)\n",
    "\n",
    "# the way to find duplicate rows is to use the duplicated method\n",
    "# this will return a boolean series\n",
    "# the first occurrence of the row is marked as False and the subsequent occurrences are marked as True\n",
    "# to get the duplicate rows, you can use the boolean series to filter the rows\n",
    "# this will return all the duplicate rows\n",
    "print(stores_df.duplicated())\n",
    "\n",
    "# identifying and dropping duplicate rows\n",
    "# duplicated method returns a boolean series\n",
    "# drop_duplicates method returns a new df with the duplicate rows removed\n",
    "# to drop the duplicates in place, use the inplace parameter\n",
    "# by default it remove the duplicates based on all the columns\n",
    "# to remove duplicates based on specific columns, use the subset parameter\n",
    "# this will remove the duplicates based on the store_nbr and city columns\n",
    "stores_df.drop_duplicates(subset=['store_nbr', 'city'],keep='last', inplace=True)\n",
    "print(stores_df.shape)\n",
    "\n",
    "# finding unique values\n",
    "# nunique method returns the number of unique values\n",
    "print(stores_df.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Filtering frame values\n",
    "# filtering the rows based on a condition\n",
    "# the condition should be a boolean series\n",
    "# Filtering a single row using loc function\n",
    "print(stores_df.head(2))\n",
    "print(stores_df.loc[stores_df.store_nbr == 11, ['store_nbr', 'city']]) # loc also takes a projection of columns\n",
    "# Filtering multiple rows using loc function\n",
    "print(stores_df.loc[stores_df.store_nbr.isin([11, 12]), ['store_nbr', 'city']])\n",
    "# Filtering the rows based on multiple conditions\n",
    "# the conditions should be combined using the bitwise operators\n",
    "# the conditions should be enclosed in parentheses\n",
    "cond = (stores_df.store_nbr == 11) | (stores_df.store_nbr == 12)\n",
    "print(stores_df.loc[cond, ['store_nbr', 'city']])\n",
    "\n",
    "# filter using where method\n",
    "# where method returns a new df with the rows that satisfy the condition\n",
    "# the rows that do not satisfy the condition are replaced with NaN\n",
    "# to drop the rows that do not satisfy the condition, use the dropna method\n",
    "print(stores_df.where(stores_df.store_nbr > 20).dropna().count())\n",
    "\n",
    "# sorting the data frame\n",
    "# sort_values method is used to sort the df\n",
    "# by default the sorting is done in ascending order\n",
    "stores_df.sort_values(by=['store_nbr'], ascending=False, inplace=True)\n",
    "\n",
    "# airthmatic column creation\n",
    "# you can create a new column by performing arithmetic operations on the existing columns\n",
    "stores_df['new_column'] = stores_df.store_nbr * 10\n",
    "\n",
    "# map method\n",
    "# a dict can be passed to the map method to create a new column\n",
    "# stores_df['new_column'] = stores_df.store_nbr.map({11: 110, 12: 120})\n",
    "stores_df['new_column'] = stores_df.store_nbr.map(lambda x: x * 100)\n",
    "\n",
    "# apply method\n",
    "stores_df['new_column'] = stores_df.apply(lambda x: x.store_nbr * 100, axis=1)\n",
    "print(stores_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catagorical data type\n",
    "- what is the difference between the categorical data type and the object data type?\n",
    "- the categorical data type is more memory efficient\n",
    "- the categorical data type is faster than the object data type for certain operations\n",
    "- It stores text data with repeating values as efficiently\n",
    "- python maps each unique cataegory to an integer to save space\n",
    "- As a rule of thumb only consider this data type when unique catagories < number of rows/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "cat_df = stores_df.sort_values(by=['store_nbr'], ascending=False)\n",
    "\n",
    "# to convert a column to a categorical data type, use the astype method\n",
    "stores_df['city'] = stores_df['city'].astype('category')\n",
    "print(stores_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating and Reshaping DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv('/Users/anand/learning/python/pylearn/notebooks/datascience/pandas/transactions.csv')\n",
    "#transactions_df.info(memory_usage='deep')\n",
    "\n",
    "# Reduce the memory usage of the dataframe\n",
    "transactions_df['date'] = pd.to_datetime(transactions_df['date'])\n",
    "transactions_df['store_nbr'] = transactions_df['store_nbr'].astype('int8')\n",
    "transactions_df['transactions'] = transactions_df['transactions'].astype('int16')\n",
    "\n",
    "#transactions_df.info(memory_usage='deep')\n",
    "\n",
    "# Find the transactions done by each store in each year\n",
    "# Adding year column to the dataframe\n",
    "transactions_df['year'] = transactions_df['date'].dt.year\n",
    "# you can aggregate the dataframe columns by using the aggregation methods\n",
    "# group store numbers by transactions\n",
    "# as_index=False will not make the multi-level index\n",
    "by_tran_df = transactions_df.groupby(['store_nbr',\n",
    "                                       'year'], as_index=False,\n",
    "                                         dropna=True).agg({'transactions': 'sum'}).sort_values(by=['transactions'],\n",
    "                                                                                                ascending=False)\n",
    "# by_tran_df_another_way = transactions_df.groupby(['store_nbr'], dropna=True)['transactions'].sum()\n",
    "\n",
    "# using multi-index aggregation\n",
    "by_tran_multi_df = transactions_df.groupby(['store_nbr',\n",
    "                                      'year'],\n",
    "                                     dropna=True).agg({'transactions': 'sum'}).sort_values(by=['transactions'],\n",
    "                                                                                           ascending=False)\n",
    "#print(by_tran_multi_df.loc[(44, 2014),:])\n",
    "\n",
    "store_tran_df = pd.read_csv(\n",
    "    '/Users/anand/learning/python/pylearn/notebooks/datascience/pandas/project_transactions.csv',)\n",
    "print(store_tran_df.head(2))\n",
    "# What is total and mean discount for each store in each week?\n",
    "# this will create multi-index columns\n",
    "disc_df = store_tran_df.groupby(['STORE_ID', 'WEEK_NO'],dropna=True).agg({\n",
    "    'COUPON_DISC': ['sum', 'mean']})\n",
    "\n",
    "print(disc_df.head(2))\n",
    "# dropping the coupon_disc index column\n",
    "disc_df = disc_df.droplevel(0, axis=1)\n",
    "print(disc_df.head(5))\n",
    "\n",
    "# you can also use tubles in the agg method to specify the column and the aggregation function\n",
    "# In this case multi-index will not be created and there is no need to drop it.\n",
    "disc_df = store_tran_df.groupby(\n",
    "    ['STORE_ID', 'WEEK_NO'], dropna=True).agg(disc_sum=('COUPON_DISC', 'sum'), disc_mean=('COUPON_DISC', 'mean'))\n",
    "\n",
    "# pivot table\n",
    "# They are good for creating summary of the data\n",
    "# pivot_table method is used to create a pivot table\n",
    "# use case: find the total discount for each store in each week\n",
    "# Adding margin=True will add a new column that will contains the row and column level statistics\n",
    "pv_df = store_tran_df.pivot_table(index='STORE_ID', columns='WEEK_NO', values='COUPON_DISC', aggfunc='sum',margins=True)\n",
    "print(pv_df.dropna().shape)\n",
    "\n",
    "# Melting dataframes\n",
    "# melt method is used to unpivot the dataframes\n",
    "# it converts columns into rows. This is great for turning wide dataframes into long dataframes\n",
    "melting_df = store_tran_df.melt(id_vars=['STORE_ID', 'WEEK_NO'], value_vars=['COUPON_DISC', 'COUPON_MATCH_DISC'])\n",
    "print(melting_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv('/Users/anand/learning/python/pylearn/notebooks/datascience/pandas/transactions.csv', parse_dates=['date'])\n",
    "#transactions_df.set_index('date').loc['2013-01', 'transactions'].plot()\n",
    "# transactions_df.plot(x='date', y='transactions')\n",
    "#transactions_df_44 = transactions_df.loc[transactions_df.store_nbr == 44, ['date', 'transactions']]\n",
    "#transactions_df_44.set_index('date').plot()\n",
    "\n",
    "# title='Transactions for store 44 and 47', -> title of the plot\n",
    "# xlabel='Year', -> x-axis label\n",
    "# ylabel='Transactions', -> y-axis label\n",
    "#     color=['red', 'blue'], style=['-', '-.'] -> color and style of the lines\n",
    "# legend=True -> show the legend\n",
    "# loc='best' -> location of the legend\n",
    "# bbox_to_anchor=(1.05, 1) -> To set the custom location of the legend\n",
    "# we can set the template for the chart globally\n",
    "plt.style.use('ggplot')\n",
    "# seaborn can also be used to set the style\n",
    "trans_44_47 = transactions_df.where((transactions_df.store_nbr == 44) | (transactions_df.store_nbr == 47)).dropna(\n",
    ").pivot_table(index='date',\n",
    "               columns='store_nbr',\n",
    "                 values='transactions',\n",
    "                   aggfunc='sum').dropna().plot(title='Transactions for store 44 and 47',\n",
    "                                                                                        xlabel='Year',\n",
    "                                                                                        ylabel='Transactions',\n",
    "                                                                                            color=['red', 'blue'],\n",
    "                                                                                              style=['-', '-.'],\n",
    "                                                                                                legend=True).legend(['store 44', 'store 47'],loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# subplots: alloes to create multiple plots in a single figure\n",
    "sales_df = pd.read_csv(\n",
    "    '/Users/anand/learning/python/pylearn/notebooks/datascience/pandas/retail_2016_2017.csv', parse_dates=['date'])\n",
    "# layout=(2, 2) -> 2 rows and 2 columns\n",
    "# sharex=True -> share the x-axis\n",
    "# sharey=True -> share the y-axis\n",
    "# figsize=(10, 10) -> size of the figure\n",
    "# subplots=True -> create subplots\n",
    "sales_df.set_index('date').plot(subplots=True, layout=(2, 2),sharey=True, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Bar charts\n",
    "# bar chart is used to show the comparison between the categories\n",
    "# compare the sales for store number 44 and 47\n",
    "# you need to specify kind=bar in the plot method or plot.bar method barh to make the bar plot horizontal\n",
    "store_in_40s = list(range(40, 50))\n",
    "transactions_df.query('store_nbr in @store_in_40s').dropna().groupby('store_nbr').agg(\n",
    "    {'transactions': 'sum'}).sort_values(by='transactions', ascending=False).plot.bar(xlabel='Store Number',\n",
    "                                                                      ylabel='Transactions',\n",
    "                                                                        title='Total transactions for stores in 40s', color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# groupd and stacked bar charts\n",
    "# set pandas to display the numbers in regular format\n",
    "sales_df.query('store_nbr in @store_in_40s').dropna().pivot_table(index='store_nbr', values='sales',\n",
    "                                                                  columns='family', aggfunc='sum').apply(lambda x: x * 100/sum(x)).plot.bar(legend=False, figsize=(20, 5))\n",
    "# Now stacking the bar chart with different family\n",
    "sales_df.query('store_nbr in @store_in_40s').dropna().pivot_table(index='store_nbr',\n",
    "                                                                   values='sales',\n",
    "                                                                   columns='family',\n",
    "                                                                  aggfunc='sum').apply(lambda x: x * 100/sum(x)).plot.bar(legend=False,\n",
    "                                                                                                                           figsize=(20, 10),\n",
    "                                                                                                                             stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# pie chart\n",
    "# pie chart is used to show the proportion of the categories\n",
    "# compare the sales for store number 44 and 47\n",
    "# you need to specify kind=pie in the plot method\n",
    "# autopct='%1.1f%%' -> to show the percentage on the pie chart\n",
    "# shadow=True -> to add shadow to the pie chart\n",
    "# startangle=90 -> to rotate the pie chart\n",
    "# explode=(0.1, 0) -> to explode the first slice\n",
    "# transactions_df.query('store_nbr in @store_in_40s').dropna().groupby('store_nbr').agg(\n",
    "#     {'transactions': 'sum'}).sort_values(by='transactions', ascending=False).plot.pie(y='transactions',\n",
    "#                                                                                       autopct='%1.1f%%',\n",
    "#                                                                                         shadow=True,\n",
    "#                                                                                           startangle=90, legend=False, figsize=(10, 10))\n",
    "# find which family has the most sales out of beverages, alcohol, and grocery\n",
    "sales_3_df = sales_df.query('store_nbr in @store_in_40s').dropna().groupby('family').agg({'sales': 'sum'}).sort_values(\n",
    "    by='sales', ascending=False).iloc[0:5,:].plot.pie(y='sales',autopct='%1.1f%%',shadow=True,startangle=90, legend=False, figsize=(5, 5))\n",
    "#sales_3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# scatter plot\n",
    "# scatter plot is used to show the relationship between two numerical variables\n",
    "# compare the transactions for store number 44 and 47\n",
    "# you need to specify kind=scatter in the plot method\n",
    "(transactions_df.query('store_nbr in @store_in_40s')\n",
    ".pivot_table(index=transactions_df.date.dt.month, columns='store_nbr', values='transactions', aggfunc='sum')\n",
    ".plot.scatter(x=44, y=47, title='Store 44 vs 47 transactions', xlabel='Store 44 transactions', ylabel='Store 47 transactions')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Histogram\n",
    "# histogram is used to show the distribution of the numerical variable\n",
    "# compare the transactions for store number 44 and 47\n",
    "# you need to specify kind=hist in the plot method\n",
    "# (\n",
    "#     transactions_df.query('store_nbr in @store_in_40s')\n",
    "#     .pivot_table(index='store_nbr', values='transactions', aggfunc='sum')\n",
    "#     .plot.hist(bins=10, title='Transactions distribution for stores in 40s', xlabel='Transactions', ylabel='Frequency')\n",
    "#  )\n",
    "transactions_df.loc[:, 'transactions'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Importing and exporting data\n",
    "# read_csv method is used to read the csv file\n",
    "# If a column name is not present in the csv then make the header=0 and pass a list of cols in the read_csv method\n",
    "# use_cols -> the coulmns to be read for example there are 10 columns and you want to read only 5 columns\n",
    "# names -> the names of the columns\n",
    "# na_values -> the values that should be treated as missing values\n",
    "# converters -> a dict that contains the column name and the function that should be applied to each value of the column\n",
    "oils_df = pd.read_csv('/Users/anand/learning/python/pylearn/notebooks/datascience/pandas/oil.csv',\n",
    "                       header=0,\n",
    "                      names=['date', 'oil_pricess'], skiprows=[0], index_col='date', parse_dates=True, na_values=['.', 'NaN'], converters={'oil_pricess': lambda x: x if x !='NaN' else 0.0})\n",
    "print(oils_df.dtypes)\n",
    "print(oils_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading from a text file\n",
    "read_csv can be used with a custom delimiter\n",
    "\n",
    "Reading from an excel file\n",
    "read_excel method is used to read the excel file\n",
    "\n",
    "to_csv and to_excel methods are used to write the dataframes to csv and excel files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sqlAlchamy is used to connect to the sql database\n",
    "- read_sql method is used to create a data frame from the sql query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# joining dataframes\n",
    "# Appending Tables: stacks th rows from multiple data frames\n",
    "# joining: add related columns from one data frame to another based on common values\n",
    "# concat method is used to append the dataframes.\n",
    "retail_16_17_df = pd.read_csv('/Users/anand/learning/python/pylearn/notebooks/datascience/pandas/retail_2016_2017.csv', parse_dates=['date'])\n",
    "automitive_df = retail_16_17_df.query('family == \"AUTOMOTIVE\"')\n",
    "beverages_df = retail_16_17_df.query('family == \"BEVERAGES\"')\n",
    "# append the dataframes\n",
    "# ignore_index=True will reset the index of the appended dataframe\n",
    "appended_df = pd.concat([automitive_df, beverages_df], ignore_index=True)\n",
    "\n",
    "# joining dataframes\n",
    "# dfs should have a at least one common column to merge on\n",
    "# syntax is left_df.merge(right_df, how='inner', left_on, right_on)\n",
    "# how: inner, outer, left, right\n",
    "# left_on: the column to join on the left dataframe\n",
    "# right_on: the column to join on the right dataframe\n",
    "# the default is inner join\n",
    "\n",
    "joined_df = transactions_df.merge(\n",
    "    retail_16_17_df, how='inner', left_on=['store_nbr', 'date'], right_on=['store_nbr', 'date'])\n",
    "# another way to join the dataframes\n",
    "# is to use just use the jeft_index=True or right_index=True and pandas will join the dataframes based on the index\n",
    "joined_another_df = transactions_df.merge(retail_16_17_df, left_index=True, right_index=True)\n",
    "joined_another_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
