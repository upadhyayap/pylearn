{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas DataFrame\n",
    "- It is an equvivalant of excel of sql table in pandas\n",
    "- it is a collection of series\n",
    "- It has two axes row and column. row axes is denoted by 0 and column axes is denoted by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4],\n",
    "    'B': [10, 20, 30, 40],\n",
    "    'C': [100, 200, 300, 400]\n",
    "})\n",
    "\n",
    "# print the dataframe\n",
    "print(df)\n",
    "\n",
    "# creating a dataframe by reading a csv file\n",
    "df = pd.read_csv('/Users/anand/learning/python/pylearn/notebooks/datascience/pandas/oil.csv',)\n",
    "# removing rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "df.shape\n",
    "# verifying that there is no missing value in the dataframe\n",
    "df.isna().sum()\n",
    "print(df.count())\n",
    "print(\"good oil\")\n",
    "good_oil = df.where(lambda x: x.dcoilwtico >= 50.0).dropna()\n",
    "print(good_oil.count())\n",
    "\n",
    "bad_oil = df.where(lambda x: x.dcoilwtico < 50.0).dropna()\n",
    "print(\"bad oil\")\n",
    "print(bad_oil.count())\n",
    "\n",
    "print(\"mean of good oil\")\n",
    "newDf = good_oil.drop(columns=['date'], axis=1)\n",
    "print(newDf.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "properties of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# properties of the dataframe\n",
    "print(f'shape: {df.shape}')\n",
    "print(f'columns: {df.columns}')\n",
    "print(f'index: {df.index}')\n",
    "print(f'dtypes: {df.dtypes}')\n",
    "print(f'axeses: {df.axes}')\n",
    "# print(\"First 2 rows\")\n",
    "# print(df.head(2))\n",
    "# print(\"Last 2 rows\")\n",
    "# print(df.tail(2))\n",
    "# print(\"random 2 samples\")\n",
    "# print(df.sample(2))\n",
    "print(\"Info\")\n",
    "print(df.info(show_counts=True))\n",
    "print(\"statistics about the data\")\n",
    "print(df.describe(percentiles=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], include='all').round())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df columns can be accessed using the dot notation as well\n",
    "# column names should follow the python variable naming rules for the dot notation to work\n",
    "stores_df = pd.read_csv('/Users/anand/learning/python/pylearn/notebooks/datascience/pandas/stores.csv')\n",
    "print(stores_df.head(2))\n",
    "\n",
    "# you can also slice the df using the column names\n",
    "subStoresdf = stores_df[['store_nbr', 'cluster']].iloc[0:5]\n",
    "print(subStoresdf.head(2))\n",
    "\n",
    "# you can rename the columns by modifying the columns attribute\n",
    "stores_df.columns = ['store_number', 'City_name', 'state', 'store_type', 'cluster']\n",
    "print(stores_df.head(2))\n",
    "\n",
    "# you can also rename the columns using the rename method\n",
    "stores_df.rename(columns={'store_number': 'store_nbr', 'City_name': 'city'}, inplace=True)\n",
    "print(stores_df.head(2))\n",
    "\n",
    "# iloc for df\n",
    "# iloc is used to access the rows and columns by their integer index\n",
    "# iloc[row_index, column_index]\n",
    "# iloc is exclusive of the end index\n",
    "print(stores_df.iloc[0:2, 0:2])\n",
    "# loc is used to access the rows and columns by their labels\n",
    "# loc[row_label, column_label]\n",
    "# loc is inclusive of the end index\n",
    "print(stores_df.loc[0:2, 'store_nbr':'state'])\n",
    "\n",
    "# drop columns\n",
    "# drop columns using the drop method\n",
    "# drop method returns a new df with the columns dropped\n",
    "# to drop the columns in place, use the inplace parameter\n",
    "stores_df.drop(columns=['state', 'store_type'], inplace=True)\n",
    "print(stores_df.head(2))\n",
    "# droppping first 10 rows\n",
    "# generally rows are dropped using slicing\n",
    "stores_df.drop(range(10), axis= 0, inplace=True)\n",
    "# duplicate the last row\n",
    "stores_df = pd.concat([stores_df, stores_df.tail(1)])\n",
    "print(stores_df.shape)\n",
    "\n",
    "# the way to find duplicate rows is to use the duplicated method\n",
    "# this will return a boolean series\n",
    "# the first occurrence of the row is marked as False and the subsequent occurrences are marked as True\n",
    "# to get the duplicate rows, you can use the boolean series to filter the rows\n",
    "# this will return all the duplicate rows\n",
    "print(stores_df.duplicated())\n",
    "\n",
    "# identifying and dropping duplicate rows\n",
    "# duplicated method returns a boolean series\n",
    "# drop_duplicates method returns a new df with the duplicate rows removed\n",
    "# to drop the duplicates in place, use the inplace parameter\n",
    "# by default it remove the duplicates based on all the columns\n",
    "# to remove duplicates based on specific columns, use the subset parameter\n",
    "# this will remove the duplicates based on the store_nbr and city columns\n",
    "stores_df.drop_duplicates(subset=['store_nbr', 'city'],keep='last', inplace=True)\n",
    "print(stores_df.shape)\n",
    "\n",
    "# finding unique values\n",
    "# nunique method returns the number of unique values\n",
    "print(stores_df.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering frame values\n",
    "# filtering the rows based on a condition\n",
    "# the condition should be a boolean series\n",
    "# Filtering a single row using loc function\n",
    "print(stores_df.head(2))\n",
    "print(stores_df.loc[stores_df.store_nbr == 11, ['store_nbr', 'city']]) # loc also takes a projection of columns\n",
    "# Filtering multiple rows using loc function\n",
    "print(stores_df.loc[stores_df.store_nbr.isin([11, 12]), ['store_nbr', 'city']])\n",
    "# Filtering the rows based on multiple conditions\n",
    "# the conditions should be combined using the bitwise operators\n",
    "# the conditions should be enclosed in parentheses\n",
    "cond = (stores_df.store_nbr == 11) | (stores_df.store_nbr == 12)\n",
    "print(stores_df.loc[cond, ['store_nbr', 'city']])\n",
    "\n",
    "# filter using where method\n",
    "# where method returns a new df with the rows that satisfy the condition\n",
    "# the rows that do not satisfy the condition are replaced with NaN\n",
    "# to drop the rows that do not satisfy the condition, use the dropna method\n",
    "print(stores_df.where(stores_df.store_nbr > 20).dropna().count())\n",
    "\n",
    "# sorting the data frame\n",
    "# sort_values method is used to sort the df\n",
    "# by default the sorting is done in ascending order\n",
    "stores_df.sort_values(by=['store_nbr'], ascending=False, inplace=True)\n",
    "\n",
    "# airthmatic column creation\n",
    "# you can create a new column by performing arithmetic operations on the existing columns\n",
    "stores_df['new_column'] = stores_df.store_nbr * 10\n",
    "\n",
    "# map method\n",
    "# a dict can be passed to the map method to create a new column\n",
    "# stores_df['new_column'] = stores_df.store_nbr.map({11: 110, 12: 120})\n",
    "stores_df['new_column'] = stores_df.store_nbr.map(lambda x: x * 100)\n",
    "\n",
    "# apply method\n",
    "stores_df['new_column'] = stores_df.apply(lambda x: x.store_nbr * 100, axis=1)\n",
    "print(stores_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catagorical data type\n",
    "- what is the difference between the categorical data type and the object data type?\n",
    "- the categorical data type is more memory efficient\n",
    "- the categorical data type is faster than the object data type for certain operations\n",
    "- It stores text data with repeating values as efficiently\n",
    "- python maps each unique cataegory to an integer to save space\n",
    "- As a rule of thumb only consider this data type when unique catagories < number of rows/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = stores_df.sort_values(by=['store_nbr'], ascending=False)\n",
    "\n",
    "# to convert a column to a categorical data type, use the astype method\n",
    "stores_df['city'] = stores_df['city'].astype('category')\n",
    "print(stores_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating and Reshaping DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   household_key    BASKET_ID  DAY  PRODUCT_ID  QUANTITY  SALES_VALUE  \\\n",
      "0           1364  26984896261    1      842930         1         2.19   \n",
      "1           1364  26984896261    1      897044         1         2.99   \n",
      "\n",
      "   STORE_ID  RETAIL_DISC  WEEK_NO  COUPON_DISC  COUPON_MATCH_DISC  \n",
      "0     31742          0.0        1          0.0                0.0  \n",
      "1     31742         -0.4        1          0.0                0.0  \n",
      "                 COUPON_DISC     \n",
      "                         sum mean\n",
      "STORE_ID WEEK_NO                 \n",
      "1        5               0.0  0.0\n",
      "         6               0.0  0.0\n",
      "                  sum  mean\n",
      "STORE_ID WEEK_NO           \n",
      "1        5        0.0   0.0\n",
      "         6        0.0   0.0\n",
      "         14       0.0   0.0\n",
      "21       8        0.0   0.0\n",
      "22       32       0.0   0.0\n",
      "(44, 103)\n",
      "(4292622, 4)\n"
     ]
    }
   ],
   "source": [
    "transactions_df = pd.read_csv('/Users/anand/learning/python/pylearn/notebooks/datascience/pandas/transactions.csv')\n",
    "#transactions_df.info(memory_usage='deep')\n",
    "\n",
    "# Reduce the memory usage of the dataframe\n",
    "transactions_df['date'] = pd.to_datetime(transactions_df['date'])\n",
    "transactions_df['store_nbr'] = transactions_df['store_nbr'].astype('int8')\n",
    "transactions_df['transactions'] = transactions_df['transactions'].astype('int16')\n",
    "\n",
    "#transactions_df.info(memory_usage='deep')\n",
    "\n",
    "# Find the transactions done by each store in each year\n",
    "# Adding year column to the dataframe\n",
    "transactions_df['year'] = transactions_df['date'].dt.year\n",
    "# you can aggregate the dataframe columns by using the aggregation methods\n",
    "# group store numbers by transactions\n",
    "# as_index=False will not make the multi-level index\n",
    "by_tran_df = transactions_df.groupby(['store_nbr',\n",
    "                                       'year'], as_index=False,\n",
    "                                         dropna=True).agg({'transactions': 'sum'}).sort_values(by=['transactions'],\n",
    "                                                                                                ascending=False)\n",
    "# by_tran_df_another_way = transactions_df.groupby(['store_nbr'], dropna=True)['transactions'].sum()\n",
    "\n",
    "# using multi-index aggregation\n",
    "by_tran_multi_df = transactions_df.groupby(['store_nbr',\n",
    "                                      'year'],\n",
    "                                     dropna=True).agg({'transactions': 'sum'}).sort_values(by=['transactions'],\n",
    "                                                                                           ascending=False)\n",
    "#print(by_tran_multi_df.loc[(44, 2014),:])\n",
    "\n",
    "store_tran_df = pd.read_csv(\n",
    "    '/Users/anand/learning/python/pylearn/notebooks/datascience/pandas/project_transactions.csv',)\n",
    "print(store_tran_df.head(2))\n",
    "# What is total and mean discount for each store in each week?\n",
    "# this will create multi-index columns\n",
    "disc_df = store_tran_df.groupby(['STORE_ID', 'WEEK_NO'],dropna=True).agg({\n",
    "    'COUPON_DISC': ['sum', 'mean']})\n",
    "\n",
    "print(disc_df.head(2))\n",
    "# dropping the coupon_disc index column\n",
    "disc_df = disc_df.droplevel(0, axis=1)\n",
    "print(disc_df.head(5))\n",
    "\n",
    "# you can also use tubles in the agg method to specify the column and the aggregation function\n",
    "# In this case multi-index will not be created and there is no need to drop it.\n",
    "disc_df = store_tran_df.groupby(\n",
    "    ['STORE_ID', 'WEEK_NO'], dropna=True).agg(disc_sum=('COUPON_DISC', 'sum'), disc_mean=('COUPON_DISC', 'mean'))\n",
    "\n",
    "# pivot table\n",
    "# They are good for creating summary of the data\n",
    "# pivot_table method is used to create a pivot table\n",
    "# use case: find the total discount for each store in each week\n",
    "# Adding margin=True will add a new column that will contains the row and column level statistics\n",
    "pv_df = store_tran_df.pivot_table(index='STORE_ID', columns='WEEK_NO', values='COUPON_DISC', aggfunc='sum',margins=True)\n",
    "print(pv_df.dropna().shape)\n",
    "\n",
    "# Melting dataframes\n",
    "# melt method is used to unpivot the dataframes\n",
    "# it converts columns into rows. This is great for turning wide dataframes into long dataframes\n",
    "melting_df = store_tran_df.melt(id_vars=['STORE_ID', 'WEEK_NO'], value_vars=['COUPON_DISC', 'COUPON_MATCH_DISC'])\n",
    "print(melting_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
